{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COM2004/COM3004 - Bayesian Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2023 University of Sheffield. All rights reserved*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* To gain practical experience of Bayesian classification.\n",
    "* To see how a classifier can be evaluated using separate training and test sets.\n",
    "* To compare the performance of 'diagonal covariance’ and 'full covariance’ multivariate Gaussian models.\n",
    "* To learn how to perform 'leave-one-out’ testing.\n",
    "\n",
    "## 1. Background\n",
    "\n",
    "So far, we have covered the theory of Bayesian classification and we have looked at trivial 1-dimensional and 2-dimensional examples. In this notebook we are going to use the same techniques but apply them to a genuine 13-dimensional classification problem – wine classification!\n",
    "\n",
    "For each section, first read the instructions carefully and make sure that you understand what is being asked before typing any Python code. You will need to make use of the numpy and scipy python libraries (https://www.scipy.org/ and http://www.numpy.org/). If you are not sure how to do something try looking it up in the online documention. If you get stuck then ask a lab demonstrator for help.\n",
    "\n",
    "## 2. Introduction\n",
    "\n",
    "The data you will be using are the genuine results of chemical analyses of wines grown in the same region of Italy but produced from three different varieties of grape. The analyses have determined the quantities of 13 chemical constituents. The task is to use the results of the chemical analyses to identify which of the three grapes was used to produce an unlabeled bottle of wine.\n",
    "\n",
    "## 3. Obtaining the data\n",
    "\n",
    "The data will form a matrix with 178 rows and 14 columns. Each row is a separate sample (i.e., a different botttle of wine). The first column stores a class label (1, 2 or 3) representing the grape variety. Columns 2 to 14 are the results of the 13 chemical analyses. Note that the features are stored as columns and the samples as rows.\n",
    "\n",
    "We will first load the data into a numpy array using the nump `loadtxt` function that we have seen in previous lab classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 14)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.loadtxt(open(\"data/wines.txt\", \"r\"), delimiter=',')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also import `matplotlib` for use later in the notebook,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Notes: Training and evaluating a classifier.\n",
    "\n",
    "Using a classifier involves two stages. First, the classifier is trained using samples for which we know the correct class labels. Training is the process of estimating the classifier’s parameters, e.g., the means and variances of the data for each class, $\\omega_i$, and the class priors, $P(\\omega_i)$. Once the classifier is trained we can use it to label unknown data. Generally, however, before using the classifier, we first want to evaluate it, i.e., test it.\n",
    "\n",
    "To *test* the classifier we will use a sample of data that was not seen during training, but for which we know the correct labels. We will classify each of the test samples and compare the classifier’s output to the *known correct* label. We then measure the percentage of the data that has been classified correctly and hope to get as close to 100% as possible. We may also want to look at the samples that are classified incorrectly to see if there is any pattern to the errors (e.g., perhaps two classes are often confused).\n",
    "\n",
    "For reasons that will become clearer later in the module, it is very important that the data used for testing the classifier are not the same data that have been used for training it. So we usually start by partitioning our data into  separate (non-overlapping) training and test sets.\n",
    "\n",
    "## 5. Preparing the wine data\n",
    "\n",
    "We are going to prepare the data by first separating out the samples for each of the three classes, and then we will divide the data for each class equally between training and testing sets.\n",
    "\n",
    "To find samples belonging to class 1 we need to select the rows of the matrix for which the first column contains a 1. If the data was stored in a numpy 2-D array, `X`, we could select these rows as follows\n",
    "\n",
    "    X[X[:,0]==1,:]\n",
    "\n",
    "Use this idea to separate the data into three numpy 2-D arrays that we will call `wines1`, `wines2` and `wines3`.\n",
    "\n",
    "We want to split `wines1`, `wines2` and `wines3` into equal training and testing partitions. The easiest way is to put the odd rows in the training set and the even rows in the test set. The odd rows of an array `X` can be selected using the syntax 0::2 which iterates from 0 to the last index of a matrix in steps of 2, i.e\n",
    "\n",
    "    X[0::2, :]\n",
    "\n",
    "and for the even rows it would be\n",
    "\n",
    "    X[1::2, :]\n",
    "    \n",
    "Use this idea to make matrices called `wines1_train`, `wines1_test`, `wines2_train` etc.\n",
    "\n",
    "Finally, combine the test data for each class back into a single test data matrix by stacking the three test data matrices on top of each other. This can be done using numpy's `vstack` function,\n",
    "    \n",
    "    wines_test= np.vstack((wines1_test, wines2_test, wines3_test))\n",
    "\n",
    "(Note the double braces above).\n",
    "\n",
    "Put this all together in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 14)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your solution here \n",
    "wines1 = X[X[:,0]==1,:]\n",
    "wines2 = X[X[:,0]==2,:]\n",
    "wines3 = X[X[:,0]==3,:]\n",
    "\n",
    "wines1_test = wines1[0::2,:]\n",
    "wines1_train = wines1[1::2,:]\n",
    "\n",
    "wines2_test = wines2[0::2,:]\n",
    "wines2_train = wines2[1::2,:]\n",
    "\n",
    "wines3_test = wines3[0::2,:]\n",
    "wines3_train = wines3[1::2,:]\n",
    "\n",
    "wines_train = np.vstack((wines1_train,wines2_train,wines3_train))\n",
    "wines_test = np.vstack((wines1_test,wines2_test,wines3_test))\n",
    "wines_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training the classifier\n",
    "\n",
    "We are going to use a multivariate Gaussian distribution to represent the distribution of each of the three classes, $p(x|\\omega)$ for $\\omega$ equals 1, 2 or 3. So we need to estimate the mean vector and covariance matrix for each class. In the first instance we are going to assume that the features are uncorrelated. This means that the Gaussian covariance matrix will have 0s for all elements off the diagonal. This is no doubt a poor assumption but in means that our 13 by 13 covariance matrix only contains 13 variances that need estimating rather than 91 parameters (why 91 not 13x13?). Note, diagonal covariance matrices make the probability evaluations very quick to compute and are often used in classification systems, and they can work well even if they do not model the data exactly.\n",
    "\n",
    "We can estimate the 13-element mean vector for each class using numpy's `mean` function. Note, you should only use columns 2 to 14 – remember, column 1 stores the class label. (Refer to the previous notebook if you have forgotten how to use `np.mean` to compute the means of each column of a matrix.) Store the results in variables called `mean1`, `mean2` and `mean3`, i.e., a separate mean vector for each class of wine.\n",
    "\n",
    "We can estimate the variances (i.e., the elements along the diagonal of the covariance matrix) using the `np.var` function. Store the results in vectors called `var1`, `var2`, and `var3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here \n",
    "mean1 = np.mean(wines1_train[:,1:] , axis=0)\n",
    "mean2 = np.mean(wines2_train[:,1:] ,axis=0 )\n",
    "mean3 = np.mean(wines3_train[:,1:] , axis=0)\n",
    "var1 = np.var(wines1_train[:,1:] , axis=0)\n",
    "var2 = np.var(wines2_train[:,1:] , axis=0)\n",
    "var3 = np.var(wines3_train[:,1:] , axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluating the classifier\n",
    "\n",
    "Once you have estimated means and variances for each class, the training stage is complete. You now need to use your classifier to process the test data and to compare the classifier outputs with the known test data labels.\n",
    "\n",
    "To perform an actual classification we need to compute $p(x|\\omega_1).P(\\omega_1)$, $p(x|\\omega_2).P(\\omega_2)$ and $p(x|\\omega_3).P(\\omega_3)$ and see which gives the highest score. We will assume that the prior probabilities are equal, i.e., $P(\\omega_1) = P(\\omega_2) = P(\\omega_3) = \\frac{1}{3}$. We can therefore perform classification simply by comparing $p(x|\\omega_1)$, $p(x|\\omega_2)$ and $p(x|\\omega_3)$.\n",
    "\n",
    "To evaluate the multivariate Gassian pdf $p(x|\\omega)$ for some known $x$ we can use the python function `multivariate_normal`. This is part of the Python `scipy.stats` module. To use it we first import it using\n",
    "    \n",
    "    from scipy.stats import multivariate_normal\n",
    "    \n",
    "We then construct a distribution object, `dist1`, with our chosen values for the mean and covariance parameters\n",
    "\n",
    "    dist1 = multivariate_normal(mean=mean1, cov=np.diag(var1))\n",
    "    \n",
    "In the above example we have constructed the covariance matrix to be a diagonal matrix with the values in the vector `var1` along the diagonal (i.e., the variances that we estimated during the training stage). \n",
    "\n",
    "Finally, to evalute the distribution for a given value of the feature vector $x$ you use,\n",
    "\n",
    "    p1 = dist1.pdf(x)\n",
    "    \n",
    "In the line above, $x$ can be a single feature vector, or a whole series of feature vectors stored in the rows of the data matrix, e.g., to evalutate the pdf for every wine sample in the test data,\n",
    "\n",
    "    p1 = dist1.pdf(wines_test[:, 1:])\n",
    "\n",
    "So now we are going to perform these evaluations for each of the three classes, i.e., using `mean1`, `var1`; then `mean2`, `var2` and finally, `mean3`, `var3`. To give us arrays of probabilities `p1`, `p2` and `p3`.\n",
    "\n",
    "The three arrays `p1`, `p2` and `p3` will each have as many elements as there are samples in the test data, say $n$. \n",
    "\n",
    "Then, using `np.vstack` we will now form these outputs, `p1`, `p2` and `p3` into a single matrix, `p`, with $n$ columns and 3 rows, i.e. `p1` in the first row, `p2` in the second and `p3` in the third. i.e.,\n",
    "\n",
    "    p = np.vstack((p1, p2, p3))\n",
    "    \n",
    "Put all of this together in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 90)\n"
     ]
    }
   ],
   "source": [
    "# Write your solution here \n",
    "from scipy.stats import multivariate_normal\n",
    "dist1 = multivariate_normal(mean = mean1 , cov = np.diag(var1))\n",
    "dist2 = multivariate_normal(mean = mean2 , cov = np.diag(var2))\n",
    "dist3 = multivariate_normal(mean = mean3 , cov = np.diag(var3))\n",
    "p1 = dist1.pdf(wines_test[:,1:])\n",
    "p2 = dist2.pdf(wines_test[:,1:])\n",
    "p3 = dist3.pdf(wines_test[:,1:])\n",
    "\n",
    "p = np.vstack((p1,p2,p3))\n",
    "# print(np.concatenate(([p1],[p2],[p3]),axis=0).shape)\n",
    "print(p.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe about the output? Can you spot the classification errors?\n",
    "\n",
    "We now need to compare the class labels that the classifier has output (`labels`) with the true labels that are stored in the first column of `wines_test`. We will then count how many labels match the true labels and divide by the total number of samples in the test set. Finally multiply by 100 get the result in terms of 'percentage correct'.\n",
    "\n",
    "What percentage would you expect to classify correctly if you had just guessed the label? Is your classifier doing better than this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correct rate is 97.77777777777777 %\n"
     ]
    }
   ],
   "source": [
    "# Write your solution here\n",
    "correctNum =  np.sum(wines_test[:,0] == w )\n",
    "incorrectNum = np.sum(wines_test[:,0] != w)\n",
    "correctRate = correctNum/(incorrectNum + correctNum)\n",
    "print( f'The correct rate is {correctRate * 100} %' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Improving the statistical model\n",
    "\n",
    "Our diagonal covariance model was quick to compute and to evaluate but it is a poor model of the true distribution of the data. Some of the features are quite highly correlated. We will now repeat the process but this time using a full covariance model. This only requires a minor change to the previous code, so we should be able to put it together quickly by cutting and pasting from previous code cells.\n",
    "\n",
    "Rather than estimate a vector of variances using `np.var`, you can now estimate the full 13x13 covariance matrix using the `np.cov` function, i.e., to estimate the covariance matrix for `wines1_train` data,\n",
    "\n",
    "    cov1 = np.cov(wines1_train[:, 1:], rowvar=0)\n",
    "\n",
    "Estimate the covariance matrix for each of `wines1_train`, `wines2_train` and `wines3_train`, storing the results in `cov1`, `cov2` and `cov3`.\n",
    "\n",
    "Testing is just the same as before but now you simply pass `cov1`, `cov2` and `cov3` as the final parameter of function `multivariate_normal` instead of `var1`, `var2` and `var3`.\n",
    "\n",
    "Repeat the multivariate normal evaluations and compute a new `p` array. Use `argmax` again to get the output labels. Score the new labels by matching against the correct test set labels.\n",
    "\n",
    "Put the code together in the cell below. Has performance improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90,) (90,) (90,)\n",
      "(270,)\n",
      "(3, 90)\n",
      "The correct rate is 98.88888888888889 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write your solution here \n",
    "\n",
    "cov1 = np.cov(wines1_train[:,1:] , rowvar = 0)\n",
    "cov2 = np.cov(wines2_train[:,1:] , rowvar = 0)\n",
    "cov3 = np.cov(wines3_train[:,1:] , rowvar = 0)\n",
    "\n",
    "\n",
    "dist1 = multivariate_normal(mean = mean1 , cov = cov1)\n",
    "dist2 = multivariate_normal(mean = mean2 , cov = cov2)\n",
    "dist3 = multivariate_normal(mean = mean3 , cov = cov3)\n",
    "p1 = dist1.pdf(wines_test[:,1:])\n",
    "p2 = dist2.pdf(wines_test[:,1:])\n",
    "p3 = dist3.pdf(wines_test[:,1:])\n",
    "print(p1.shape,p2.shape,p3.shape)\n",
    "p = np.vstack((p1,p2,p3))\n",
    "print(np.concatenate(([p1],[p2],[p3]),axis=0).shape)\n",
    "print(p.shape)\n",
    "\n",
    "w = np.argmax(p, axis=0) + 1\n",
    "plt.plot(w)\n",
    "\n",
    "correctNum =  np.sum(wines_test[:,0] == w )\n",
    "incorrectNum = np.sum(wines_test[:,0] != w)\n",
    "correctRate = correctNum/(incorrectNum + correctNum)\n",
    "print( f'The correct rate is {correctRate * 100} %' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. More accurate evaluation\n",
    "\n",
    "The problem with this task is that after splitting the data into training and test sets there is barely sufficient data to accurately estimate the classifier parameters (remember, from Lab 2, that sample means and variances estimates have a good chance of being inaccurate when $N$ is small). Also there is not really enough data for accurately evaluating the classifier: just 1 extra error changes the performance evaluation by more than 1%.\n",
    "\n",
    "Ideally we want to be able to use all the data for training and all the data for testing – but it was said earlier that we need to keep training and testing data separate, i.e., testing on data that has been used during training does not produce valid results. \n",
    "\n",
    "So what is the solution? How can we use all the data for training, all the data for testing *and* having training and test sets that do not overlap?\n",
    "\n",
    "The solution is to do something called ‘leave-one-out’ testing. In this approach we use just the first sample for testing and train using all the remaining $N-1$ samples. But then we repeat the exercise using the 2nd sample for testing and the other $N-1$ for training, and then again using the 3rd sample for testing and so on until we have tested all $N$ samples. We can then report the combined result of all $N$ tests.\n",
    "\n",
    "The obvious downside to this is that we now have to train $N$ different classifiers, i.e. each classifier is trained using the full set of data but with a different test sample omitted. However computers are good at doing repetitive things, so for small problems leave-one-out testing is a practical approach.\n",
    "\n",
    "By using a loop we can iterate through the data samples and call a function that tests a single target sample while training on the remaining $N-1$ samples. The code is below but you need to complete the fuction `test_one_out`.\n",
    "\n",
    "Complete the code below and use it to re-evaluate the classifier using both the diagonal and full-covariance models. \n",
    "\n",
    "What are your new results and how do they compare to those you had earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.43820224719101\n"
     ]
    }
   ],
   "source": [
    "def test_one_out(data, test_index):\n",
    "    # This function should train a classifier using all the data in\n",
    "    # 'data' except the sample test_index. It should then test the\n",
    "    # sample test_index. And return True if the sample is classified\n",
    "    # correctly or else return False\n",
    "    # COMPLETE THE CODE\n",
    "    #\n",
    "    #\n",
    "    # print(data.shape)\n",
    "    test_data = data[test_index,:]\n",
    "    # print(test_data.shape)\n",
    "    train_data = np.concatenate((data[:test_index],data[test_index+1:,]))\n",
    "    # print(train_data.shape)\n",
    "    train1 = train_data[train_data[:,0]==1]\n",
    "    train2 = train_data[train_data[:,0]==2]\n",
    "    train3 = train_data[train_data[:,0]==3]\n",
    "\n",
    "    mean1 = np.mean(train1[:,1:] , axis=0)\n",
    "    mean2 = np.mean(train2[:,1:] , axis=0)\n",
    "    mean3 = np.mean(train3[:,1:] , axis=0)\n",
    "\n",
    "    cov1 = np.cov(train1[:,1:] , rowvar= 0 )\n",
    "    cov2 = np.cov(train2[:,1:] , rowvar= 0 )\n",
    "    cov3 = np.cov(train3[:,1:] , rowvar= 0 )\n",
    "\n",
    "    dist1 = multivariate_normal(mean = mean1, cov = cov1)\n",
    "    dist2 = multivariate_normal(mean = mean2, cov = cov2)\n",
    "    dist3 = multivariate_normal(mean = mean3, cov = cov3)\n",
    "\n",
    "    p1 = dist1.pdf(test_data[1:])\n",
    "    p2 = dist2.pdf(test_data[1:])\n",
    "    p3 = dist3.pdf(test_data[1:])\n",
    "    \n",
    "    p = np.concatenate(([p1],[p2],[p3]),axis = 0 )\n",
    "\n",
    "    index = np.argmax(p) + 1\n",
    "    correct = index == data[test_index, 0]\n",
    "  \n",
    "    return correct\n",
    "\n",
    "def classify(data):\n",
    "    \"\"\"Classify every sample using leave-one-out training\"\"\"\n",
    "    ncorrect = 0\n",
    "    ntotal = data.shape[0]\n",
    "    for index in range(ntotal):\n",
    "        ncorrect = ncorrect + test_one_out(data, index)\n",
    "    percent_correct = ncorrect * 100.0 / ntotal\n",
    "    return percent_correct\n",
    "    \n",
    "print(classify(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Selecting features\n",
    "\n",
    "Say that you were told that the current wine classification system was too expensive to run. Performing 13 separate chemical analyses is taking too long and the chemists want to reduce the number of tests down to 6. Which 6 features would you choose to ensure best performance?\n",
    "\n",
    "Notice that the code you have written is equally valid for tasks of any dimensionality, ie., the constant 13 should not have to appear anywhere in the code. So you could evaluate a 6-D version of the classifier simply by selecting the label column and 6 additional columns as the very first step of the program, e.g.,\n",
    "    \n",
    "    wines6D = wines[:, [0, 1, 4, 6, 8, 9, 12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 7)\n",
      "93.82022471910112\n"
     ]
    }
   ],
   "source": [
    "# e.g. feature 1, 4, 6, 8, 9, 12 will allow a performance of 93.8%\n",
    "wines6D = X[:, [0, 1, 4, 6, 8, 9, 12]]\n",
    "print(wines6D.shape)\n",
    "print(classify(wines6D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 6)\n",
      "99.43820224719101\n"
     ]
    }
   ],
   "source": [
    "# Try experimenting with different choices of features here.\n",
    "wines5D = X[:, [0, 1, 3, 7, 11, 13]]\n",
    "print(wines5D.shape)\n",
    "print(classify(wines5D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge**: Can you find 6 features that perform almost as well as the full 13?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 2nd half of the module we will be looking at some techniques for solving this problem but for now either use trial and error or try looking at the histograms of individual features to find dimensions along which the classes appear to be well separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best feature is [ 1  3  4  5  7 11], and correct rate is 99.43820224719101\n"
     ]
    }
   ],
   "source": [
    "max_corr = 0\n",
    "max_feature = []\n",
    "\n",
    "for i in range (1000):\n",
    "    features = np.sort( np.random.choice(range(1, 13), 6, replace=False))\n",
    "    wines6Feature = X[:, [0,*features]]\n",
    "    if classify(wines6Feature) > max_corr : \n",
    "        max_corr = classify(wines6Feature)\n",
    "        max_feature = features\n",
    "\n",
    "print(f'The best feature is {max_feature}, and correct rate is {max_corr}')\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true,
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
